\section{Описание предметной области, анализ и выбор методов ре-
шения задач}
\label{sec:software_design}

\subsection{Теоретические предпосылки и альтернативные парадигмы нечёткой логики}
\label{subsec:theory_foundations_expand}

Нечёткая логика возникла как ответ на ограниченность классической двухзначной модели истинности при описании реального мира, где многие понятия имеют «размытые» границы. Ниже изложены основные философско–методологические предпосылки, критические альтернативы и ключевые расширения, легшие в основу современного аппарата нечётких методов.

\subsubsection{Философско-онтологические основания}
\begin{itemize}
  \item \textbf{Лингвистический континуум.} Уже у Аристотеля (IV в. до н. э.) встречается понимание «срединных понятий»: «теплый», «большой», «молодой»— все они описывают непрерывный континуум, а не дискретные состояния.
  \item \textbf{Логики многозначности.} В начале XX в. Ян Łukasiewicz и Haskell Curry формализовали трёх- и многозначные логики, вводя третье значение «неопределено».
  \item \textbf{Эпистемологическая неопределённость.} Работы C. Pérez и Н. de Ramón в 1950–60-х по философии языка подчёркивали, что человеческое знание по своей природе неполно и подвержено шуму.
  \item \textbf{Парадокс границ (Sorites).} Классическая дилемма «когда кучка песка перестаёт быть кучкой», поставленная античными философами, стала одним из мотивационных толчков к формализации «ступенчатой» принадлежности.
\end{itemize}

\subsubsection{Рождение нечётких множеств}
Лотфи Заде в работе \emph{“Fuzzy Sets”} (1965) радикально обобщил классическую теорию множеств, вводя функцию принадлежности:
\begin{equation}
  \mu_A: U \;\to\; [0,1],
  \label{eq:basic_mu}
\end{equation}
где $\mu_A(x)$ отражает «степень истинности» высказывания «$x$ принадлежит $A$». Это позволило:
\begin{itemize}
  \item учесть экспертные и лингвистические оценки («примерно 0.7»);
  \item моделировать шум и неопределённость измерений;
  \item задавать правила с «мягким» срабатыванием.
\end{itemize}

\subsubsection{Альтернативные теории неопределённости}
\label{subsubsec:alt_uncertainty_expand}

Хотя нечёткая логика оказалась популярной, ряд смежных парадигм предлагают иные подходы:

\paragraph{Теория вероятностей.}  
Опирается на аддитивную меру $\mathbb{P}(A)$ и статистические законы, хорошо подходит для описания случайных процессов, но не отражает семантической «нечёткости» понятий.

\paragraph{Теория доказательств Демпстера–Шеффера.}  
Вводит функцию массы $m(A)$ для каждой подсистемы доказательств; степень веры $Bel(A)$ и допустимости $Pl(A)$ моделируют комбинацию экспертной информации.

\paragraph{Грубые множества (Rough Sets).}  
Павловский (1982) формализовал нижнюю и верхнюю аппроксимации классов, не требующие функций принадлежности, но отражающие границы неопределённости через отношения эквивалентности.

\paragraph{Интервальные множества.}  
Определяют значение не как точечное, а как интервал $[a,b]$, что аналогично non–singleton идее, но без лингвистического наполнения.

\paragraph{Теория возможностей.}  
Заде (1978) предложил пары функции возможности $\Pi(A)=\sup_{x\in A}\mu(x)$ и необходимости $N(A)=1-\Pi(\bar A)$, моделирующие степень одобрения и степень обязательности.

\subsubsection{Развитие алгебраического аппарата}
\label{subsubsec:algebraic_tools}

\paragraph{$t$- и $s$-нормы (1971–73).}  
Обобщают операции $\land,\lor$:
\begin{align}
  T(a,b)&\in\{\min(a,b),\,a\cdot b,\,\max(0,a+b-1)\},\\
  S(a,b)&\in\{\max(a,b),\,a+b-ab,\,\min(1,a+b)\}.
\end{align}
Каждая норма обеспечивает различные свойства ассоциативности, коммутативности и монотонности.

\paragraph{Интегралы Чуэко и Сугено (1989–90).}  
Агрегируют функции $f(x)$ с нечёткой мерой $\nu$:
\[
  \mathrm{Choquet}(f)=\sum_{i=1}^n\bigl(f(x_{(i)})-f(x_{(i-1)})\bigr)\,\nu(A_{(i)}),
\]
\[
  \mathrm{Sugeno}(f)=\max_i\min\bigl(f(x_{(i)}),\nu(A_{(i)})\bigr).
\]
Их применяют в многокритериальном анализе и оценке рисков.

\subsubsection{Фаззи-парадигмы вывода}
\label{subsubsec:inference_expand}

\paragraph{Mamdani (1974).}  
Правила используют лингвистические консеквенты; процедура:
\begin{enumerate}
  \item Фазификация singleton-входов.
  \item Вычисление степени каждого правила $w_k=\min_i\mu_{A^k_i}(x_i)$.
  \item Агрегация выходных множеств $\mu_C(y)=\max_k\min(w_k,\mu_{B^k}(y))$.
  \item Дефазификация центроидом.
\end{enumerate}

\paragraph{Takagi–Sugeno (1985).}  
Консеквент $f_k(\mathbf{x})$ — аналитическая функция (линейная или полиномиальная), итог:
\[
  y=\frac{\sum_k w_k f_k(\mathbf{x})}{\sum_k w_k}.
\]

\paragraph{Tsukamoto (1983).}  
Требует монотонных MF и выполняет дефазификацию каждого правила через обратную функцию.

\subsubsection{Type-2 FS и non–singleton input}
\label{subsubsec:type2_nsi_expand}

\paragraph{Type-2 FS (2001).}  
Каждая MF сама является нечёткой: $\mu_{\tilde A}(x,u)\in[0,1]$, $u\in J_x$ — третий параметр. Решение через type-reduction (алгоритм Karnik–Mendel).

\paragraph{Non-singleton вход (Mendel \& Nguyen, 2001).}  
Вход представляется MF $\mu_X(x)$ вместо $\delta$-функции:
\[
  \alpha_k=\sup_xT\bigl(\mu_X(x),\mu_{A_k}(x)\bigr),\quad
  y^*=\frac{\iint y\,\mu_Y(y|x)\,\mu_X(x)dxdy}{\iint\mu_Y(y|x)\,\mu_X(x)dxdy}.
\]

\subsubsection{Когнитивные карты и динамические системы}
\label{subsubsec:cognitive_maps_expand}

Фаззи-когнитивные карты (FCM) моделируют причинно-следственные цепочки:
\[
  A^{t+1}=f\bigl(A^tW\bigr),\quad A\in[0,1]^n,\;W\in[-1,1]^{n\times n}.
\]
Используются для прогнозирования эволюции сложных систем (социальные сети, экология).

\subsubsection{Гибридные подходы}
\label{subsubsec:hybrids_expand}

\paragraph{ANFIS (1993).}  
Комбинирует TSK-вывод с обучением градиентом:
\begin{itemize}
  \item MF-параметры обучаются backprop.
  \item Линейные консеквенты — решаются через LS.
\end{itemize}

\paragraph{Neuro-Evolution.}  
Эволюционные алгоритмы (GA, PSO, DE) для совместного оптимального поиска структуры правил и MF.

\paragraph{Deep Fuzzy Architectures.}  
Интеграция фазфи-сабвейеров перед CNN и Transformer, адаптивные нечёткие внимания в графовых нейронных сетях.

\subsubsection{Альтернативы и комплементарные теории}
\label{subsubsec:complementary}

\begin{itemize}
  \item \textbf{Rough Sets.} Нижняя/верхняя аппроксимации для обработки дискретных неопределённостей.
  \item \textbf{D–S Theory.} Комбинация доказательств через правила плёзора и веры.
  \item \textbf{Possibility Theory.} Критические меры $\Pi,N$ для шкалирования нечетких событий.
  \item \textbf{Grey Systems.} Моделируют «серые» зоны неопределённости через интервалы без MF.
\end{itemize}


\subsection{Основные математические понятия}
\label{subsec:math_basics}

\paragraph{Нечёткое множество.}
Пусть $U$ — универсум дискурса. Нечёткое множество
$A\subseteq U$ задаётся функцией принадлежности
\begin{equation}
  \mu_A : U \;\to\; [0,1],
  \label{eq:mf_def}
\end{equation}
где значение $\mu_A(u)$ интерпретируется как «степень, в которой
$u$ принадлежит $A$».

\paragraph{Альфа-срезы и теорема о представлении.}
Для $\alpha\in(0,1]$ \emph{$\alpha$-срез} множества $A$
определяется как
\begin{equation}
  A_\alpha = \bigl\{u\in U \mid \mu_A(u)\ge\alpha\bigr\}.
  \label{eq:alpha_cut}
\end{equation}
Теорема Zadeh гласит: $A$ полностью определяется семьёй
своих $\alpha$-срезов, упорядоченных по $\alpha$.

\paragraph{Расширенный принцип (extension principle).}
Если $g:U\to V$ — отображение классических множеств,
то образ нечёткого $A$ определяется как
\begin{equation}
  \mu_{g(A)}(v)=\sup_{u\in g^{-1}(v)} \mu_A(u).
  \label{eq:extension}
\end{equation}
Формула \eqref{eq:extension} лежит в основе вывода
и дефаззификации.

\paragraph{Сравнение с теорией вероятностей.}
Вероятностная мера $\mathbb P$ описывает
\emph{стохастическую} неопределённость
(сколько раз выпадет орёл),
тогда как $\mu_A$ отражает
\emph{семантическую/лингвистическую} неопределённость
(насколько объект «высокий»).
Фактически, вероятность удовлетворяет
аддитивности, а~функция принадлежности~— нет.

\subsection{Классификация систем вывода и их свойства}
\label{subsec:classification}

Полный процесс нечеткого вывода состоит из четырёх этапов:

\begin{enumerate}
  \item \emph{Фазификация}: $u\mapsto\mu_{A_i}(u)$
  \item \emph{Агрегация условий}: $t$-норма/ко-норма,
        например, произведение
        \begin{equation}
          w_k=\prod_{i=1}^{n}\mu_{A_{ik}}(x_i);
          \label{eq:tprod}
        \end{equation}
  \item \emph{Композиция правил}: максимум или сумма
        результирующих множеств;
  \item \emph{Дефаззификация}: превращение
        нечёткого вывода в скаляр.  
        Для правила Мамдани чаще всего —
        центроид
        \begin{equation}
          y^\star=\frac{\displaystyle
                     \int_{\mathbb R} y\,\mu_Y(y)\,dy}
                     {\displaystyle\int_{\mathbb R}\mu_Y(y)\,dy}.
          \label{eq:centroid}
        \end{equation}
\end{enumerate}

Табл.~\ref{tab:sys_comp} суммирует различия ключевых архитектур.

\begin{table}[h]
\centering\small
\caption{Сравнение систем нечёткого вывода}
\label{tab:sys_comp}
\begin{tabular}{@{}p{0.21\linewidth}p{0.57\linewidth}p{0.14\linewidth}@{}}
\toprule
\textbf{Тип} & \textbf{Особенности} & \textbf{Плюсы / минусы} \\ \midrule
Мамдани & Лингв. консеквент; деф.\,по~\eqref{eq:centroid} &
          + интерпретируемость; – интеграл дорог \\[2pt]
TSK (Sugeno) & Консеквент $f_k(\mathbf x)$, итог~— \eqref{eq:tprod}–средневес & + быстро; – хуже объяснимость \\[2pt]
ANFIS & TSK как сеть 5 слоёв & + градиент; – \#параметров $\uparrow$ \\[2pt]
Type-2 & MF второго порядка & + робустность; – type-reduction O($n^2$) \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Подход \emph{non–singleton} и его обоснование}
\label{subsec:nsi_deep}

Пусть измерение $x$ снабжено приборной погрешностью $\sigma$.
Тогда вместо точки берём гауссово нечёткое число
\begin{equation}
  \mu_{X}(u)=\exp\!\Bigl(
    -\tfrac12\bigl(\tfrac{u-x}{\sigma}\bigr)^{\!2}\Bigr).
  \label{eq:gauss_nsi}
\end{equation}
Активность $k$-го правила вычисляют суп-композицией
\begin{equation}
  w_k=\sup_{u\in U}
      \bigl[\mu_{X}(u)\,\boldsymbol{\ast}\,
            \mu_{A_k}(u)\bigr],
  \label{eq:nsi_sup}
\end{equation}
где $\boldsymbol{\ast}$ — $t$-норма (обычно $t_{\min}$ или Prod).
В~результате интеграл дефаззификации (для Мамдани) преобразуется в
\begin{equation}
  y^\star=\frac{\displaystyle
      \iint\limits_{U\times\mathbb R}
      y\,
      \mu_Y(y\,|\,u)\,
      \mu_{X}(u)\,dy\,du}
    {\displaystyle
      \iint\limits_{U\times\mathbb R}
      \mu_Y(y\,|\,u)\,
      \mu_{X}(u)\,dy\,du},
  \label{eq:nsi_defuzz}
\end{equation}
что приводит к росту сложности до $O(nmP)$, где $P$ —
кол-во точек дискретизации. Этот факт обусловливает
\emph{необходимость} GPU-ускорения и оптимизирующих библиотек.

\subsection{Современные исследовательские тренды}
\label{subsec:trends}

\textbf{Type-2 $\to$ Type-3.}\,
Недавно предложен класс \emph{General Type-3 Fuzzy Sets}
(Bustince et al., 2023), расширяющий неопределённость
ещё на уровень ядра MF.

\textbf{Fuzzy Deep Learning.}\,
Интеграция нечётких слоёв в CNN/Transformer
для повышения робустности к «adversarial noise»
и объяснимости
(Chen et al., 2024).

\textbf{Эволюционная оптимизация.}\,
Алгоритмы NSGA-III, CMA-ES
дают парето-фронт «точность – сложность»
для отсева избыточных правил и MF.

\textbf{Edge AI.}\,
Лёгкие TSK-модели с quantization-aware training
компилируются в WebAssembly
для IIoT-датчиков (Nikita et al., 2025)


\subsection{Выводы доменного анализа}
\label{subsec:conclusion}

\begin{enumerate}
  \item \textbf{Нечёткая логика} предоставляет формальный аппарат
        для моделирования лингвистической неопределённости;
        её зрелость подтверждена более чем 50-летней практикой.
  \item \textbf{Non–singleton input} — естественное расширение,
        адекватно учитывающее шум сенсоров и экспертные интервалы,
        но требующее интегральных вычислений \eqref{eq:nsi_defuzz}.
  \item \textbf{Пробел} рынка — отсутствие открытого инструмента,
        комбинирующего NSI, GPU-ускорение и AutoML-тюнинг.
  \item Разработка исследовательского фреймворка
        \emph{«нейронечёткая система с non-singleton входами»}
        удовлетворяет актуальный спрос академии и индустрии,
        а также способствует продвижению Explainable AI
        в области высоконадежных систем.
\end{enumerate}

\begin{itemize}
  \item \textbf{MATLAB Fuzzy Toolbox} — богатый GUI, но
        нет возможности кастомного backprop,
        лицензия $\approx$ 1200 год.
  \item \textbf{scikit-fuzzy} — отсутствие GPU
        и поддержки \eqref{eq:nsi_sup}–\eqref{eq:nsi_defuzz};
        конфигурация MF через ini-файлы мало-гибкая.
  \item \textbf{jFuzzyLogic} — последний релиз 2016 г.;
        Java 8, зависимость от ANTLR 2.7.
  \item \textbf{anfis-torch} — интересное proof-of-concept,
        но покрывает только 1-D MF, нет non-singleton
        и функций дефаззификации, кроме weighted-average.
\end{itemize}

\subsection{Цель разработки}
\label{subsec:goal}

Целью работы является создание \textbf{нейронечёткой системы с non--singleton-входами}, обеспечивающей полный цикл экспериментирования: от загрузки и предобработки пользовательских данных до настройки правил, оптимизации гиперпараметров и выбора метода дефаззификации.  
Основные функциональные требования сформулированы в табл.~\ref{tab:req}. В последующих подразделах для \emph{каждого} требования перечислены реализуемые подходы, их преимущества, ограничения и иллюстративные примеры кода на~\texttt{Python}.

\begin{table}[h]
\centering\small
\caption{Функциональные требования к системе}
\label{tab:req}
\begin{tabular}{p{0.25\linewidth}p{0.65\linewidth}}
\toprule
Требование & Краткое описание \\
\midrule
\textbf{Загрузка данных} & Импорт CSV, Parquet, Excel; поддержка больших наборов с ленивой обработкой (Dask, Polars).\\
\textbf{Предобработка} & Ресэмплирование, нормализация, балансировка, удаление выбросов.\\
\textbf{Выбор термов} & Настраиваемое число и тип термов для каждого признака.\\
\textbf{Выбор классов} & Задание числа классов и их семантических меток.\\
\textbf{База правил} & Конфигурирование вида правил и начальных параметров.\\
\textbf{Границы поиска} & Определение диапазонов для оптимизируемых параметров.\\
\textbf{Оптимизация} & \texttt{Optuna}; в перспективе~--- генетический алгоритм, Bayesian Optimization и~др.\\
\textbf{Функции принадлежности} & Поддержка различных семейств~MF для антецедентов и консеквентов.\\
\textbf{Дефаззификация} & Центроид, биссектор, MOM, SOM, LOM, высота и др. \\
\bottomrule
\end{tabular}
\end{table}



%-----------------------------------------------------------------
\subsection{Нейронные сети}
\label{subsec:neural_nets}

Нейронная сеть (Neural Network, NN) состоит из последовательности слоёв,
каждый из которых преобразует входной вектор $\mathbf{a}^{(l-1)}$ в
выходной $\mathbf{a}^{(l)}$ по правилу
\begin{equation}
  \mathbf{z}^{(l)} = W^{(l)}\mathbf{a}^{(l-1)} + \mathbf{b}^{(l)}, 
  \quad
  \mathbf{a}^{(l)} = \sigma\bigl(\mathbf{z}^{(l)}\bigr),
  \label{eq:nn_forward_full}
\end{equation}
где $W^{(l)}\in\mathbb{R}^{n_l\times n_{l-1}}$, 
$\mathbf{b}^{(l)}\in\mathbb{R}^{n_l}$,
а $\sigma(\cdot)$ — элементная активация (ReLU, sigmoid, tanh).

\paragraph{Ключевые архитектуры}
\begin{itemize}
  \item \textbf{MLP (многослойный перцептрон)} — последовательность полносвязных слоёв.
  \item \textbf{CNN (сверточная сеть)} — сверточные и пулинг–слои для обработки изображений:
    \[
      y_{i,j}^{(l)} = \sum_{u=-k}^k\sum_{v=-k}^k
      K_{u,v}^{(l)}\,x_{i+u,j+v}^{(l-1)} + b^{(l)}.
      \label{eq:cnn_full}
    \]
  \item \textbf{RNN/LSTM} — рекуррентные связи для работы с последовательностями:
    \begin{equation}
      h_t = \phi\bigl(W_{xh}x_t + W_{hh}h_{t-1} + b_h\bigr),
      \quad
      y_t = W_{hy}h_t + b_y.
      \label{eq:rnn_full}
    \end{equation}
  \item \textbf{Transformer} — self-attention и позиционное кодирование:
    \[
      \mathrm{Attention}(Q,K,V) = \mathrm{softmax}\!\bigl(\tfrac{QK^\top}{\sqrt{d_k}}\bigr)\,V.
      \label{eq:attention_full}
    \]
\end{itemize}

\paragraph{Обучение}
Оптимизация параметров $\theta=\{W^{(l)},\mathbf{b}^{(l)}\}$ происходит
градентным спуском по функции потерь $\mathcal{L}(\theta)$:
\begin{equation}
  \theta \leftarrow \theta - \eta\,\nabla_\theta \mathcal{L}(\theta),
  \label{eq:gd_full}
\end{equation}
где обычно применяют Adam \cite{Kingma2015}:
\begin{align}
  m_t &= \beta_1 m_{t-1} + (1-\beta_1)\,\nabla\mathcal{L},\quad
  v_t = \beta_2 v_{t-1} + (1-\beta_2)\,(\nabla\mathcal{L})^2, \nonumber\\
  \hat m_t &= m_t/(1-\beta_1^t),\quad
  \hat v_t = v_t/(1-\beta_2^t),\quad
  \theta_{t+1} = \theta_t - \eta\,\hat m_t/(\sqrt{\hat v_t}+\epsilon).
  \label{eq:adam_full}
\end{align}

\paragraph{Пример кода (PyTorch)}
\begin{lstlisting}[language=Python, caption={Простейший цикл обучения MLP на PyTorch}, label={lst:nn_training}]
import torch, torch.nn as nn, torch.optim as optim

class MLP(nn.Module):
    def __init__(self, dims):
        super().__init__()
        layers = []
        for i in range(len(dims)-1):
            layers += [nn.Linear(dims[i], dims[i+1]), nn.ReLU()]
        self.net = nn.Sequential(*layers[:-1])
    def forward(self, x): return self.net(x)

model = MLP([input_dim, 64, 32, output_dim]).to(device)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=1e-3)

for epoch in range(epochs):
    optimizer.zero_grad()
    preds = model(X_train)
    loss = criterion(preds, y_train)
    loss.backward()
    optimizer.step()
\end{lstlisting}

%-----------------------------------------------------------------
\subsubsection{Методы обучения нейросетей и нечётких систем}
\label{subsec:training_methods}

\paragraph{Нейронные сети.}
\begin{itemize}
  \item \textbf{Градиентный спуск} (SGD, Adam, RMSprop) — быстрый локальный поиск.
  \item \textbf{Regularization} (Dropout, BatchNorm, weight decay) для борьбы с переобучением.
  \item \textbf{Transfer learning} — дообучение предобученных моделей.
  \item \textbf{Meta-learning / AutoML} (HyperOpt, Optuna) для поиска архитектуры и гиперпараметров.
\end{itemize}

\paragraph{Нечёткие системы.}
\begin{itemize}
  \item \textbf{Экспертное задание MF и правил} — ручная настройка на основе знаний.
  \item \textbf{Алгоритмы типа Wang–Mendel} \cite{Wang1992} для автоматической генерации правил.
  \item \textbf{Эволюционные алгоритмы} (GA, PSO, Differential Evolution) для оптимизации MF и весов правил.
  \item \textbf{Градиентное обучение MF} в ANFIS — backprop через параметры функций принадлежности.
  \item \textbf{Bayesian/BO–тюнинг} (Optuna) — поиск границ MF и структуры базы правил.
\end{itemize}

\paragraph{Сравнительный анализ методов обучения}
\begin{center}
\begin{tabular}{@{}p{0.25\linewidth}p{0.33\linewidth}p{0.33\linewidth}@{}}
\toprule
\textbf{Метод} & \textbf{Нейросети} & \textbf{Нечёткие системы} \\ \midrule
Градиентный & 
Backprop, Adam, быстрый on-line & 
ANFIS: backprop через MF, редко применяется  \\[2pt]
Эволюционный & 
редко (NeuroEvolution) &
GA/PSO для MF + правил, глобальный поиск \\[2pt]
Bayesian/BO & 
Optuna, Ax, Hyperopt для гиперпарам. &
Optuna для MF, границ правил; медленнее \\[2pt]
Экспертный ввод & 
нет (данные → модель) &
основа для MF, структур правил \\[2pt]
AutoML & 
AutoKeras, AutoGluon &
ограниченно: FuzzyAutoML проекты \\ 
\bottomrule
\end{tabular}
\end{center}

%-----------------------------------------------------------------
\paragraph{Детальное сравнение нейросетей и нечёткой логики}
\label{subsec:nn_vs_fuzzy_expanded2}

\begin{table}[h]
\centering\small
\caption{Сравнение по методам обучения и свойствам}
\label{tab:nn_fuzzy_training}
\begin{tabular}{@{}p{0.20\linewidth}p{0.36\linewidth}p{0.36\linewidth}@{}}
\toprule
\textbf{Аспект} & \textbf{Нейросети} & \textbf{Нечёткая логика} \\ \midrule
Инициализация & случайные веса, pretrain & заранее заданные MF, пороговые значения \\[4pt]
Обновление & градиентный шаг \eqref{eq:gd_full} & GA-прогрессия / Optuna Trial \\[4pt]
Сходимость & зависит от lr, batch size, architecture & зависит от структуры правил, параметров MF \\[4pt]
Глобальный поиск & сложен, требует NeuroEvolution & GA/PSO встроены, естественно \\[4pt]
Автоматизация & AutoML фреймворки & Optuna, но меньше готовых решений \\[4pt]
Интерпретируемость  & пост-хок (LIME, SHAP) & прямая: правила–последствия \\[4pt]
Оценка качества & валидация на hold-out & экспертная и статистическая валидация \\ 
\bottomrule
\end{tabular}
\end{table}

\paragraph{Заключение.}
\begin{itemize}
  \item \emph{Нейросети} полагаются на мощные процедуры градиентного обучения и AutoML, но требуют больших данных и пост-хок интерпретации.
  \item \emph{Нечёткие системы} используют комбинацию экспертных знаний и глобального поиска (GA, BO), обеспечивая читаемость, но уступая в зрелости AutoML-сред.
  \item Гибридные подходы (ANFIS, SugenoGPU) берут лучшее из обоих миров: интерпретируемые правила и дифференцируемые MF, обучаемые градиентом, часто в сочетании с Optuna для мета-оптимизации.
\end{itemize}

\subsection{Инструментальные средства и технологии разработки}
\label{subsec:tools}

\subsubsection{Актуальность выбора инструментов}
\label{subsubsec:why_tools}

При разработке нейронечёткой системы с non–singleton входами возникают
ряд взаимосвязанных требований к среде разработки и используемым библиотекам:
необходимость эффективной обработки больших объёмов данных,
высокая вычислительная производительность при сложных интегральных операциях,
а также гибкость в экспериментировании и воспроизводимость результатов.
Одновременно важна прозрачность и удобство отладки, а также лёгкость
интеграции компонентов в единую платформу.

\medskip

\noindent Основные аргументы в пользу продуманного выбора стека:
\begin{itemize}
  \item \textbf{Производительность.} Фазификация non–singleton входов
        требует многократных операций над массивами данных и
        интегральных вычислений с высокой степенью параллелизма.
        Без GPU-ускорения и оптимизированных библиотек
        время вычисления становится неприемлемым.
  \item \textbf{Гибкость.} Экспериментальная платформа должна
        позволять динамически менять архитектуру нейронечёткой модели,
        добавлять новые функции принадлежности, заменять оптимизаторы
        и изменять логику дефаззификации без переписывания кода «с нуля».
  \item \textbf{Воспроизводимость.} Поддержка управления зависимостями
        (виртуальные окружения, Docker), фиксированные версии библиотек
        и единый конфигурационный формат гарантируют
        идентичность получаемых результатов на разных машинах.
  \item \textbf{Экосистема.} Доступ к большим сообществам разработчиков,
        обширная документация и постоянная поддержка обновлений
        являются залогом долгосрочной устойчивости проекта.
\end{itemize}

\subsubsection{Менеджмент окружения и зависимостей}

Для контроля версий пакетов и изоляции окружения рекомендуются:
\begin{itemize}
  \item \textbf{Conda / mamba.} Гибкое создание виртуальных сред,
        возможность установки как Python-библиотек, так и
        низкоуровневых CUDA-драйверов в одном канале.
  \item \textbf{Poetry.} Современный инструмент для управления
        зависимостями и публикации пакетов, поддерживает lock-файл
        и разрешает конфликтующие версии.
  \item \textbf{Docker.} Контейнер с базовым образом \verb|nvidia/cuda|,
        позволяющий запускать приложение на любом GPU-сервере
        без дополнительной настройки.
\end{itemize}

\subsubsection{Ключевые библиотеки и примеры использования}

\paragraph{Работа с данными: pandas, polars, Dask.}
\bigskip
\emph{pandas} остаётся де-факто стандартом для анализа табличных данных:
интуитивный API, богатый набор функций агрегации, ресэмплирования
и временных индексов.  
Для больших наборов данных применимы \emph{polars} (Rust-ядро,
комбинация ленивых вычислений и multithreading) и \emph{Dask}
(распределённая обработка на кластере).

\begin{lstlisting}[language=Python, caption={Пример ресэмплирования временных рядов в pandas}, label={lst:pandas_resample}]
import pandas as pd
df = pd.read_parquet("sensor_data.parquet")
df.index = pd.to_datetime(df["timestamp"])
df_resampled = df.resample("100ms").mean().interpolate()
\end{lstlisting}

\paragraph{Численные расчёты: NumPy, SciPy, CuPy, Numba.}

\emph{NumPy} обеспечивает базовые операции над многомерными массивами.
\emph{SciPy} дополняет оптимизацией, интегрированными методами и
статистическими инструментами.  
\emph{CuPy} даёт «drop-in» замену NumPy на GPU,
а \emph{Numba} компилирует критичные фрагменты кода в машинный язык
с поддержкой CUDA-ядр.

\begin{lstlisting}[language=Python, caption={Numba-ускоренная функция для трапециевидной MF}, label={lst:numba_trap}]
from numba import njit
import numpy as np

@njit
def trap_mf(x, a, b, c, d):
    if x < a or x > d:
        return 0.0
    elif x < b:
        return (x-a)/(b-a)
    elif x <= c:
        return 1.0
    else:
        return (d-x)/(d-c)
\end{lstlisting}

\paragraph{Машинное обучение и оптимизация: PyTorch, scikit-learn, imblearn, Optuna.}
\indent \emph{PyTorch} служит ядром для тензорной алгебры, CUDA-ускорения,
автоградиента и гибкого построения вычислительных графов.
\emph{scikit-learn} предоставляет baseline-модели, трансформеры данных
и метрики качества.  
\emph{imbalanced-learn} наращивает функциональность scikit-learn 
версиями SMOTE, ADASYN и другими.  
\emph{Optuna} — современный фреймворк для гипероптимизации
с поддержкой прунинга и распределённого хранения экспериментов.

\begin{lstlisting}[language=Python, caption={Пример Pipeline с SMOTE и RandomForest}, label={lst:pipeline}]
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from imblearn.over_sampling import SMOTE

pipe = Pipeline([
    ("scale",  StandardScaler()),
    ("smote",  SMOTE(k_neighbors=4)),
    ("clf",    RandomForestClassifier(n_estimators=200))
])
pipe.fit(X_train, y_train)
\end{lstlisting}

\begin{lstlisting}[language=Python, caption={Optuna-оптимизация параметров модели}, label={lst:optuna_example}]
import optuna
def objective(trial):
    n_trees = trial.suggest_int("n_trees", 50, 300)
    max_depth = trial.suggest_int("max_depth", 5, 20)
    clf = RandomForestClassifier(n_estimators=n_trees, max_depth=max_depth)
    score = cross_val_score(clf, X_train, y_train, cv=3).mean()
    return 1 - score

study = optuna.create_study(direction="minimize")
study.optimize(objective, n_trials=50)
\end{lstlisting}

\subsubsection{Сводное сравнение инструментов}
\label{subsubsec:tool_choice}

\begin{table}[h]
\centering\small
\caption{Критерии выбора инструментов}
\label{tab:tool_choice}
\begin{tabular}{@{}p{0.16\linewidth}p{0.20\linewidth}p{0.44\linewidth}p{0.12\linewidth}@{}}
\toprule
\textbf{Категория} & \textbf{Инструмент} & \textbf{Обоснование выбора} & \textbf{Альтернатива} \\ \midrule
Язык проекта       & Python 3.12 & + обширная экосистема ML; + динамическая разработка  & C++ / Java \\[2pt]
Окружение          & Conda, Docker & + изоляция зависимостей; + портируемость & pipenv \\[2pt]
Табличные данные   & pandas, polars & + удобный API; + латентные вычисления в polars & Apache Arrow \\[2pt]
Научные вычисления & NumPy, SciPy, CuPy, Numba & + CPU/GPU ускорение; + богатая функциональность & MATLAB \\[2pt]
ML и оптимизация   & PyTorch, scikit-learn, imblearn, Optuna & + единая платформа; + AutoML & TensorFlow \\[2pt]
UI и визуализация  & Streamlit, Plotly & + быстрое прототипирование; + интерактивность & Dash, Bokeh \\[2pt]
DevOps и CI/CD     & GitHub Actions, Docker & + автоматизация тестов и сборок & Jenkins \\ 
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Выводы по подразделу}
\label{subsubsec:tool_summary}

В результате подробного обзора выбраны инструменты,
обеспечивающие:
\begin{itemize}
  \item надёжную и воспроизводимую среду (Conda, Docker, GitHub Actions);
  \item высокую производительность при фазификации и дефаззификации
        (PyTorch, CuPy, Numba);
  \item богатый набор средств для работы с данными и их балансировки
        (pandas, polars, imblearn);
  \item гибкие механизмы гипероптимизации и экспериментального
        прототипирования (Optuna, Streamlit).
\end{itemize}

Такой инструментальный стек гарантирует модульность, масштабируемость
и удобство сопровождения проекта нейронечёткой системы
с non–singleton входами.
