\section{Описание предметной области, анализ и выбор методов решения задач}

\subsection{Нечёткая логика}
\subsubsection{Базовые понятия и мотивация}
Классическая логика, исходя из дихотомии \textbf{«истина/ложь»},
оказалась неудовлетворительной сразу, как только возникли задачи,
где данные заданы в виде субъективных формулировок
(«температура \emph{высокая}», «поверхность \emph{почти гладкая}»)  
или подвержены неустранимой стохастической погрешности.
Уже первые попытки формально описать нечеткие категории
в лингвистике (работы сопоставительного анализа А. Цвики,
Дж. Лакоффа) показали, что понятие «принадлежит» носит скорее
\emph{градуальный} характер.  
Именно это обстоятельство в 1965 г. подтолкнуло Л.~Заде к введению
концепции \textbf{нечеткого множества}
$A=\{(x,\mu_A(x))\mid x\!\in\!X\}$, 
где \emph{функция принадлежности} $\mu_A\colon X\!\to\![0,1]$
отражает степень, с которой элемент~$x$ удовлетворяет
внутреннему, зачастую неформализуемому свойству «быть $A$»  
\cite{zadeh1965}.  

С самого начала теория получила ярко выраженную
прикладную направленность:
\begin{itemize}
  \item интеллектуальные регуляторы температуры (Мамдани, 1975);
  \item диагностические процедуры «человек—машина» в медицине
        и NDТ;
  \item гибрид «фаззи–нейро» (ANFIS) для адаптивного
        управления химическими процессами;
  \item экспертные системы рекомендаций «если–то»,
        использующие нечёткие правила.
\end{itemize}
Все эти применения опираются на небольшой, но выразительный набор
операций, собранных в табл.\,1 (с.~\pageref{tab:operations}).  
Разумеется, для полноты логического исчисления требуется
ещё импликация и эквиваленция, но их удобнее обсуждать вместе  
с t-/s-нормами (табл.\,2).  

Значительную роль играет гибкость \emph{семейства функций
принадлежности}: треугольная, трапециевидная, гауссова,
обобщённая Парабола Белла — каждая отражает
различное понимание «размытости» границ.
На рис.~\ref{fig:membership} эти формы даны
для равного универсума, что наглядно демонстрирует,
как изменяется компромисс «простота - аппроксимирующая
способность»:
треугольная — минимальна по вычислительным издержкам,
гауссова — наилучше согласуется со статистикой,
но требует вычислять экспоненту.

\paragraph{Мотивация выбора нечеткой логики.}
С точки зрения инженера-практика, аргументация
в пользу нечетких моделей сводится к следующим тезисам:
\begin{enumerate}
  \item \textbf{Полнота описания.}  
    Экспертные знания редко выражаются в форме
    строгих вероятностных распределений,  
    тогда как лингвистические кванторы («почти», «довольно»)  
    естественно переводятся в степени принадлежности.
  \item \textbf{Интерпретируемость.}  
    Правило вида
    «ЕСЛИ температура \emph{высокая}, ТО вентилятор \emph{ускорить}»
    прозрачно для технолога и объяснимо для регулятора.
  \item \textbf{Равномерная обработка точных и неточных данных.}  
    Чёткие величины инкапсулируются как \emph{одиночные} точки
    с $\mu\!=\!1$, сохраняя целостность математического аппарата.
  \item \textbf{Смежность с другими ИИ-подходами.}  
    Понятие функции принадлежности напрямую связано с сигмоидными
    активациями, что облегчает гибридизацию
    «нейронная сеть + фаззи» (см. дисс.\,Кулабухова \cite{kulabukhov2023}).
\end{enumerate}

\subsubsection{Базовые операции нечёткой логики}
Хотя теория размытых множеств эволюционировала в десятках направлений,
практик-инженер использует конечный репертуар \emph{атомарных} действий.  

\begin{table}[h]
\centering
\caption{Базовые операции над нечёткими множествами}
\label{tab:baseops}
\begin{tabularx}{\linewidth}{@{}p{2.8cm}p{3.2cm}X@{}}
\toprule
\textbf{Операция} & \textbf{Символ / кратко} & \textbf{Общее определение / заметки} \\ \midrule
Дополнение & $\neg A$ &
$\mu_{\neg A}(x)=N\!\bigl(\mu_A(x)\bigr)$;
чаще $N(u)=1-u$ \\[0.3em]
Пересечение & $A\cap B$ &
$\mu_{A\cap B}=T(\mu_A,\mu_B)$,
	обычно $T=\min$ или $T=ab$ \\[0.3em]
Объединение & $A\cup B$ &
$\mu_{A\cup B}=S(\mu_A,\mu_B)$,
	стандарт $S=\max$ или $S=a+b-ab$ \\[0.3em]
Разность & $A\setminus B$ &
$T\bigl(\mu_A,N(\mu_B)\bigr)$;
полезно при маскировании шумов \\[0.3em]
Импликация & $A\!\to\!B$ &
$I(\mu_A,\mu_B)=\sup\{z\mid T(\mu_A,z)\le\mu_B\}$;
остаточная к $T$ \\[0.3em]
Эквиваленция & $A\!\leftrightarrow\!B$ &
$E=1-|\mu_A-\mu_B|$;
иногда Бернулли-метрика $1-(\mu_A-\mu_B)^2$ \\[0.3em]
Модификатор «очень» & $very\,A$ &
$\mu_{very\,A}=\mu_A^{\,2}$:
сжимает хвосты, усиливая “ядро” \\[0.3em]
Модификатор «слегка» & $slightly\,A$ &
$\mu_{slightly\,A}=\sqrt{\mu_A}$:
расширяет область «почти верно» \\ \bottomrule
\end{tabularx}
\end{table}
\newpage
\subsubsection{Базовые функции принадлежности}

Каждая из этих кривых появилась исторически не «из головы»,
а как попытка уловить определённый тип субъективной информации:  
\emph{треугольная} — быстро рисуется экспертом линейкой,  
\emph{гауссова} — идеальна, если разброс данных напоминает
нормальное распределение,  
\emph{колоколообразная} и \emph{sigmoid-S/Z} пришли
из нейрокомпьютинга, где математически удобно
работать с плавным экспоненциальным хвостом.  

С позиции инженера-системотехника важно помнить:
\begin{enumerate}
  \item при одинаковой форме \emph{отличаются} параметры
        (координаты вершин, $\sigma$ и т.\,д.);
  \item ширина МПФ напрямую задаёт “толерантность” к шуму;
  \item композитные (“кусочно-гауссовы”, “полисплайн”) МПФ
        допускают \emph{любой} уровень гладкости,
        если того требует оптимизатор градиентных методов.
\end{enumerate}


Семейство МПФ велико, но на практике ~80 \% публикаций
ограничиваются шестёркой, показанной на рис.~\ref{fig:mpf}.
\begin{figure}[h]
\centering
\begin{tikzpicture}
  \begin{axis}[
    width=.9\textwidth,
    height=.45\textwidth,
    xmin=0,xmax=10,ymin=0,ymax=1.05,
    grid=both,grid style={gray!20},
    axis lines=left,
    xlabel={$x$}, ylabel={$\mu$},
    legend style={font=\small, at={(0.5,-0.25)},anchor=north,columns=3},
    samples=200
  ]
    %--- triangular
    \addplot[very thick,domain=0:4]{max(0,1-abs(x-2)/2)};
    \addlegendentry{Треугольная}
    %--- trapezoid
    \addplot[thick,domain=0:6]{max(0,min((x-1)/2,1,(6-x)/2))};
    \addlegendentry{Трапециевидная}
    %--- gaussian
    \addplot[thick,domain=0:10]{exp(-((x-7)^2)/(2*1.2^2))};
    \addlegendentry{Гауссова}
    %--- bell (generalized)
    \addplot[densely dashed,domain=0:10]{1/(1+abs((x-5)/1.3)^(2*2))};
    \addlegendentry{Колоколообразная}
    %--- sigmoid S
    \addplot[dashed,domain=0:10]{1/(1+exp(-2*(x-3)))};
    \addlegendentry{Sigmoid–S}
    %--- z-shape
    \addplot[dash dot,domain=0:10]{(x<2)?1:(x>6)?0:(1-2*((x-2)/(6-2))^2)};
    \addlegendentry{Z-форма}
  \end{axis}
\end{tikzpicture}
\caption{Классические функции принадлежности  
  (параметры выбраны произвольно для наглядности).}
\label{fig:mpf}
\end{figure}


\subsection{Современные методы нечеткого вывода и нейро-нечеткие системы}
\label{sec:advanced_inference}

Поиски оптимальных алгоритмов нечеткого вывода продолжаются: разрабатываются
новые методы дефаззификации~[28], усовершенствованные треугольные нормы~[20,29]
и операторы импликации~[14,20,21,30–32]. Эти операторы редко применяются в
эвристиках Мамдани–Ларсена–Сугено, но находят широкое использование в
интеллектуальном анализе данных, в частности в нейро-нечетких
системах~[3,4,6,7,33,34].

Нейро-нечеткие системы объединяют принципы искусственных нейронных сетей и
нечеткой логики. Их работа состоит из четырёх этапов:

\begin{enumerate}
  \item \emph{Фуззификация входов}:
    чёткие значения \(x_j\) преобразуются в степени принадлежности
    \(\mu_{A_i}(x_j)\):
    \[
      \mu_{A_i}(x_j) = A_i(x_j).
    \]
  \item \emph{Активация правил} (используя параметрическую \(t\)-норму,
    например, норму Швейцера–Склэра):
    \[
      \alpha_i
      = T_{\lambda}\bigl(\mu_{A_i}(x)\bigr),
      \quad
      T_{\lambda}(a,b)
      = \max\!\bigl(0,\;1-[(1-a)^\lambda + (1-b)^\lambda]^{1/\lambda}\bigr).
    \]
  \item \emph{Агрегация выводов} (с помощью оператора импликации –
    резидуального вида):
    \[
      \tilde{\mu}_B(y)
      = \bigvee_i I_T\bigl(\alpha_i,\mu_{B_i}(y)\bigr),
      \quad
      I_T(a,b)
      = \sup\{z\mid T(a,z)\le b\}.
    \]
  \item \emph{Дефаззификация}: чёткий результат \(y^*\) вычисляется, например,
    методом центра тяжести:
    \[
      y^*
      = \frac{\displaystyle \int y\,\tilde{\mu}_B(y)\,dy}
             {\displaystyle \int \tilde{\mu}_B(y)\,dy}.
    \]
\end{enumerate}

В \autoref{fig:centroid_defuzz} приведён пример треугольного нечеткого множества
и вычисления его центра тяжести (метод дефаззификации).

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.6\textwidth]{centroid_plot.png}
  \caption{Треугольное нечеткое множество и центр тяжести дефаззификации}
  \label{fig:centroid_defuzz}
\end{figure}

Для обучения нейро-нечетких систем используются методы настройки
весов \(w_{ij}\) и параметров функций принадлежности:
\begin{itemize}
  \item алгоритм обратного распространения ошибки (градиент первого порядка)~[4],
  \item метод Бройдена–Флетчера–Гольфарба–Шэнно (градиент второго порядка)~[3],
  \item эволюционные стратегии случайного поиска~[35].
\end{itemize}

Хотя увеличение числа обучаемых параметров повышает риск переобучения и
снижает интерпретируемость~[4], нейро-нечеткие системы демонстрируют высокую
эффективность при моделировании сложных процессов и интеллектуальном
анализе данных~[3,33].


\subsubsection{Методы нечёткого вывода}

\paragraph{1. Постановка задачи и основное определение}
Рассмотрим лингвистическую модель, состоящую из множества нечётких правил MISO-структуры:
\begin{equation}
  R_k:\quad
  \text{Если }x_1\in A_{1k}\wedge\cdots\wedge x_n\in A_{nk},\quad
  \text{то }y\in B_k,
  \label{eq:rule_general}
\end{equation}
где $A_{ik}\subseteq X_i, B_k\subseteq Y$ — нечёткие множества,
определённые функциями принадлежности
$\mu_{A_{ik}}:X_i\to[0,1]$, $\mu_{B_k}:Y\to[0,1]$.
Область входов: $\mathbf{x}=(x_1,\dots,x_n)\in X_1\times\cdots\times X_n=:X$.

\begin{definition}
Нечёткое отношение $R_k\subseteq X\times Y$ задаётся функцией
$$
  \mu_{R_k}(\mathbf{x},y)
  = I\bigl(\mu_{A_{1k}}(x_1),\dots,\mu_{A_{nk}}(x_n);\;\mu_{B_k}(y)\bigr),
$$
где $I:[0,1]^{n+1}\to[0,1]$ — неоднозначный импликатор, задающий связь
между степенями истинности посылок и заключения.
\end{definition}

\paragraph{2. Этапы классического вывода}
Нечёткий вывод традиционно проходит следующие этапы:

\begin{enumerate}
  \item \emph{Фаззификация входов.}
    Каждый числовой вход $x_i$ подвергается фаззификации:
    определяется вектор степеней
    $$
      (\mu_{A_{i1}}(x_i), \,\ldots, \,\mu_{A_{iN}}(x_i))
      \in [0,1]^N.
    $$
    Это позволяет перейти от точечного значения к распределению принадлежности.

  \item \emph{Активация правил.}
    Для каждого правила $R_k$ вычисляется степень активации
    \begin{equation}
      \alpha_k
      = T\bigl(\mu_{A_{1k}}(x_1),\dots,\mu_{A_{nk}}(x_n)\bigr),
      \label{eq:activation}
    \end{equation}
    где $T:[0,1]^n\to[0,1]$ — t-норма (обычно $\min$ или $\prod$).

  \item \emph{Инференция (применение импликации).}
    Применение импликатора $I$ даёт выходное нечеткое множество
    \begin{equation}
      \mu_{B'_k}(y)
      = I\bigl(\alpha_k,\mu_{B_k}(y)\bigr).
      \label{eq:inference}
    \end{equation}
    Наиболее часто используется
    $$
      \mu_{B'_k}(y)=\min\{\alpha_k,\mu_{B_k}(y)\}\quad(\text{Mamdani}),
    $$
    либо
    $$
      \mu_{B'_k}(y)=\alpha_k\,\mu_{B_k}(y)\quad(\text{Larsen}).
    $$

  \item \emph{Агрегация и дефаззификация.}
  \begin{itemize}
    \item Агрегация: объединение результатов всех правил
    $$
      \mu_{B'}(y)
      = \max_{1\le k\le N}\mu_{B'_k}(y).
    $$
    \item Дефаззификация: переход к числовому результату,
    например, по центру тяжести:
    \begin{equation}
      y^*
      = \displaystyle\frac{\int_Y y\,\mu_{B'}(y)\,dy}{\int_Y\mu_{B'}(y)\,dy}.
      \label{eq:defuzz_centroid}
    \end{equation}
  \end{itemize}
\end{enumerate}

\paragraph{3. Популярные варианты импликаторов и t–норм}
Подходы отличаются выбором $T$ и $I$:\
\textbf{Mamdani (1974).} $T=\min$, $I=\min$.\\
\textbf{Larsen (1980).} $T=\min$, $I(a,b)=a\cdot b$.\\
\textbf{Sugeno (1985).} Использует аналитические функции $f_k(x)$; итог:
$y^*=\sum\alpha_k y_k/\sum\alpha_k$.\\
\textbf{Tsukamoto (1979).} Монотонные функции для дефаззификации.

\paragraph{4. Доказательство полиномиальной сложности}
\begin{proof}
В классическом композиционном правиле необходимо проводить
максимум по всем комбинациям входов, что даёт $O(m^n)$ при $m$
уровнях фаззификации. В многоэтапном подходе каждая активация
\eqref{eq:activation} — $O(n)$, инференция \eqref{eq:inference} — $O(1)$,
агрегация — $O(N)$, дефаззификация — $O(m)$. Итого $O(n+N+m)$,
что полиномиально.
\end{proof}

\begin{figure}[h!]
  \centering
  \begin{tikzpicture}[scale=2.5,>=Stealth]
    \node at (-0.5,2.2){\textbf{Импликации}};
    \draw[->,ultra thick] (0,2) -- (1,2) node[midway,above]{\Large $\min$};
    \node at (1.2,2){\small Mamdani};
    \draw[->,ultra thick] (0,1) -- (1,1) node[midway,above]{\Large $\times$};
    \node at (1.2,1){\small Larsen};
  \end{tikzpicture}
  \caption{Увеличенный масштаб: минимум и произведение
    для иллюстрации импликаций.}
  \label{fig:implications_large2}
\end{figure}

\subsubsection{Нечёткое значение истинности}

\paragraph{1. Определение и мотивация}
Числовое значение истинности
\(\tau_{A/A'}\in[0,1]\)
задаётся как:
\begin{equation}
  \tau_{A/A'}
  = \sup_{x\in X}\min\{\mu_A(x),\,\mu_{A'}(x)\}.
  \label{eq:tau_def3b}
\end{equation}
Оно отражает максимальную степень одновременного
"принадлежания" $x$ множествам $A$ и $A'$.

\paragraph{2. Подробное доказательство свойств}
\begin{theorem}
Величина $\tau_{A/A'}$:\
(1) равна 1 тогда и только тогда, когда
$\mu_A(x)\le\mu_{A'}(x)$ при всех $x$;\
(2) равна 0 тогда и только тогда,
когда $\min\{\mu_A(x),\mu_{A'}(x)\}=0$ для всех $x$;\
(3) монотонна:
$A_1\subseteq A_2\implies \tau_{A_1/A'}\le \tau_{A_2/A'}$.
\end{theorem}

\begin{proof}
Из определения \eqref{eq:tau_def3b} следует,
что максимум достигается при $x$, где
$\min(\mu_A,\mu_{A'})$ максимален. Для (1):
если $\mu_A(x)\le\mu_{A'}(x)$, то
$\min(\mu_A,\mu_{A'})=\mu_A$ и
$\sup_x\mu_A(x)=1$. Обратное — аналогично.
Для (2): $\min(\mu_A,\mu_{A'})=0$ на всех $x$.
Монотонность — из неубывания $\mu_{A_1}\le\mu_{A_2}$.
\end{proof}

\begin{figure}[h!]
  \centering
  \begin{tikzpicture}[scale=2.5,>=Stealth]
    \draw[->] (0,0)--(4,0) node[right]{$x$};
    \draw[->] (0,0)--(0,2) node[above]{$\mu$};
    \draw[thick,blue,domain=0:4,samples=100]
      plot(\x,{exp(-((\x-1.3)^2)/0.8)});
    \node at (1.3,1.6){\footnotesize $\mu_A(x)$};
    \draw[thick,red,domain=0:4,samples=100]
      plot(\x,{exp(-((\x-2.7)^2)/1.1)});
    \node at (2.7,1.1){\footnotesize $\mu_{A'}(x)$};
    \fill[gray,opacity=0.4]
      plot[domain=1.6:2.4] (\x,{min(exp(-((\x-1.3)^2)/0.8),exp(-((\x-2.7)^2)/1.1))})
      -- (2.4,0) -- (1.6,0) -- cycle;
    \node at (2.0,0.5){\small перекрытие};
  \end{tikzpicture}
  \caption{Увеличенный пример: область перекрытия
    для численного $\tau_{A/A'}$.}
  \label{fig:tau_enlarged2}
\end{figure}

Логика, опирающаяся на концепцию нечеткой истинности, обычно именуется лингвистической. 
В рамках этого подхода «нечеткая истинность» рассматривается как лингвистическая переменная, задаваемая пятёркой
\[
\langle \beta,\, T,\, X,\, G,\, M\rangle,
\]
где
\begin{itemize}
  \item $\beta$ — название переменной («истинность»);
  \item $T = \{\text{абсолютно ложно, очень ложно, слегка ложно, ложно,\\
         квазиложно, квазиистинно, истинно, слегка истинно, очень истинно, абсолютно истинно}\}$
        — множество термов;
  \item $X = [0,1]$ — область значений лингвистической переменной;
  \item $G$ — синтаксическая процедура построения составных термов;
  \item $M$ — семантическая процедура, задающая функции принадлежности.
\end{itemize}
\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{images/нечеткая истинность.png}
  \caption{Треугольное нечеткое множество и центр тяжести дефаззификации}
  \label{fig:centroid_defuzz}
\end{figure}
% \subsubsection{Методы нечёткого вывода}
% \subsubsection{Нечёткое значение истинности}
% \subsubsection{Нечёткая степень истинности}

% \subsection{Проблемы исходных данных в задачах машинного обучения}
% \subsubsection{Размасштабированность и гетерогенность признаков}
% \subsubsection{Шум, выбросы и пропуски}
% \subsubsection{Несбалансированность классов}
%
% \newpage
%
% \subsection{Обзор инструментальных средств}
% \subsubsection{Выбор языка программирования: Python}
% \subsubsection{Использование GPU: CUDA и PyTorch}
% Лингвистическая логика на основе нечеткой истинности
\subsubsection{Лингвистическая логика на основе нечеткой истинности}
\label{sec:linguistic_logic}

Логика, опирающаяся на концепцию нечеткой истинности, обычно именуется лингвистической\cite{ref40,ref43,ref44}. В рамках этого подхода «нечеткая истинность» рассматривается как лингвистическая переменная:

\[
\langle \beta,\,T,\,X,\,G,\,M\rangle,
\]

где:
\begin{itemize}
  \item $\beta$ — наименование переменной («истинность»);
  \item $T$ — множество термов (лингвистических оценок):
    \{«истинно», «ложно», «очень истинно», «очень ложно», «слегка истинно», «слегка ложно», «абсолютно истинно», «абсолютно ложно», «квазиистинно», «квазиложно»\};
  \item $X = [0,1]$ — числовой интервал, по которому измеряется степень истинности;
  \item $G$ — синтаксическая процедура, задающая правила построения новых термов;
  \item $M$ — семантическая процедура, отображающая каждый терм в соответствующее нечеткое множество.
\end{itemize}

Применение такого формализма позволяет аккуратно выражать различные оттенки восприятия истинности фактов, утверждений и явлений, что особенно важно в экспертных системах и при принятии решений.

На Рисунке~\ref{fig:membership} приведены функции принадлежности значений лингвистической переменной «истинность».

\subsection{Обзор инструментальных средств}
\label{sec:tools}

\subsubsection{Выбор языка программирования: Python}
\label{sec:tools_python}

Python — язык высокого уровня с лаконичным синтаксисом. Основные преимущества:
\begin{itemize}
  \item Обширная экосистема научных библиотек: \texttt{NumPy}, \texttt{SciPy}, \\ \texttt{Pandas}, \texttt{Matplotlib}
  \item Фреймворки для машинного обучения: \texttt{scikit-learn}, \texttt{TensorFlow}, \\ \texttt{PyTorch}
  \item Быстрое прототипирование в Jupyter;
  \item Большое сообщество и документация.
\end{itemize}

\subsection{Архитектура и оптимизация GPU для обучения нейронных сетей}
\label{ssec:gpu_architecture}

Графические процессоры (GPU) изначально создавались для рендеринга трёхмерной графики, где требовалось одновременно обрабатывать миллионы пикселей. Однако, когда стало ясно, что обучение глубоких нейросетей сводится к повторению единотипных операций над большими массивами чисел, архитектура GPU оказалась почти идеальной. В отличие от CPU с его несколькими быстро работающими ядрами и громоздкой системой предсказания ветвлений, GPU использует сотни или тысячи упрощённых ядер, которые синхронно выполняют одну и ту же инструкцию над разными данными.   

Для понимания того, как они отличаются, достаточно вспомнить, что современные серверные CPU обычно имеют 8–64 полноценных ядра, каждое из которых оснащено суперскалярным исполнителем, многоуровневой кэш-иерархией (L1–L3) и сложной системой предсказания переходов. Такая структура хорошо подходит для разнородных задач — ветвящихся алгоритмов, многозадачности, работы с нерегулярными структурами данных, но она не способна раскрутить более чем десятки параллельных потоков без значительного роста стоимости и энергопотребления.  

GPU же строятся по принципу «массированного параллелизма»: независимые арифметические блоки объединяются в стриминговые мультипроцессоры (Streaming Multiprocessors, SM), а SM, в свою очередь, умножаются до сотен штук на кристалле. Потоки группируются в «warps» (в NVIDIA) или «wavefronts» (в AMD), и все нити в такой группе исполняют одну и ту же инструкцию. Внутри SM есть быстрое локальное хранилище (shared memory) и регистры, что позволяет координировать взаимодействие потоков с минимальной задержкой.  

Память GPU организована в несколько уровней:  
\begin{itemize}  
  \item \emph{Глобальная память (VRAM)} на основе GDDR6 или HBM2/3 обеспечивает пропускную способность до нескольких терабайт в секунду, но обладает заметной латентностью.  
  \item \emph{Кэш уровня L2} объединяет доступы от всех SM и снижает количество обращений к глобальной памяти.  
  \item \emph{Разделяемая память} внутри каждого SM и \emph{регистры} обеспечивают сверхнизкую латентность при обмене данными между соседними тредами.  
\end{itemize}  

Ключевым усовершенствованием последних поколений стало появление тензорных ядер (Tensor Cores), рассчитанных на смешанную точность (обычно FP16/FP32 или BF16). Они позволяют выполнять матричные умножения $4\times4$ или даже $16\times16$ за один такт, что даёт дополнительный прирост производительности в несколько раз при сохранении приемлемой точности сходимости.  

С точки зрения ПО разработчики используют платформы CUDA (NVIDIA) или HIP (AMD), где вся работа разбивается на:  
\begin{itemize}  
  \item \textbf{Запуск кернелов} — небольших программ, которые выполняются на тысячах потоков одновременно.  
  \item \textbf{Асинхронные копирования} (CUDA MemcpyAsync) — перекрытие передачи данных между CPU и GPU с вычислениями, чтобы видеокарта не простаивала.  
  \item \textbf{Стримы и графы задач} — механизм группировки операций для минимизации накладных расходов вызова ядра.  
  \item \textbf{Kernel Fusion} — объединение последовательных операций в единый кернел, позволяющее сократить число обращений к глобальной памяти.  
\end{itemize}  

На практике обучение крупных нейросетей нередко разбивают на две части:  
\begin{enumerate}  
  \item Предобработка, аугментация и загрузка данных (I/O, трансформации) выполняются на CPU, где быстрее работают ветвления и системные вызовы.  
  \item Основной проход (forward/backward) и обновление параметров — на GPU, где тысячи ядер решают одинаковые подзадачи над матрицами и тензорами.  
\end{enumerate}  

Визуализация профилей реальных задач показывает, что современные ускорители типа NVIDIA A100 обеспечивают до 1–1.5 PFLOPS в смешанной точности при потреблении порядка 300 Вт, тогда как эквивалентная нагрузка на серверный CPU (Intel Xeon) даст лишь пару сотен GFLOPS при схожем энергопотреблении. Это объясняет, почему GPU остаются ключевым ресурсом для дата-центров машинного обучения.  

Конечно, у GPU есть и ограничения:  
\begin{itemize}  
  \item \textbf{Ограниченный объём VRAM}. Модели типа GPT-3 XXL могут не помещаться на одной карте, требуя шардирования по нескольким GPU.  
  \item \textbf{Высокая латентность случайных доступов} к памяти усложняет нерегулярные вычисления и динамические графы.  
  \item \textbf{Накладные расходы} на передачу данных по PCIe/NVLink (несколько миллисекунд при больших объёмах).  
  \item \textbf{Сложность отладки} параллельных алгоритмов и профилирования (нужны специальные инструменты: Nsight, rocProfiler).  
\end{itemize}  

В ближайшие годы мы увидим всё более плотную интеграцию CPU и GPU в едином корпусе (NVIDIA Grace, AMD Instinct CDNA), а также рост популярности альтернативных ускорителей — TPU от Google, FPGA и экспериментальных оптических и нейроморфных чипов. Эти гибридные решения позволят сократить задержки передачи данных и ещё более повысить эффективность распределённых систем глубокого обучения.  


\subsubsection{Использование GPU: CUDA и PyTorch}
\label{sec:tools_gpu}

\paragraph{Модель программирования CUDA}
Программист описывает kernel-функции, которые запускаются параллельно. Пример сложения двух векторов:

\begin{lstlisting}[language=C,caption={CUDA: сложение векторов},label={lst:cuda_vecadd}]
__global__ void vecAdd(const float *A, const float *B, float *C, int N) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < N) {
        C[i] = A[i] + B[i];
    }
}

float *d_A, *d_B, *d_C;
cudaMalloc(&d_A, N * sizeof(float));
cudaMalloc(&d_B, N * sizeof(float));
cudaMalloc(&d_C, N * sizeof(float));
cudaMemcpy(d_A, h_A, N * sizeof(float), cudaMemcpyHostToDevice);

int blockSize = 256;
int gridSize  = (N + blockSize - 1) / blockSize;
vecAdd<<<gridSize, blockSize>>>(d_A, d_B, d_C, N);

cudaMemcpy(h_C, d_C, N * sizeof(float), cudaMemcpyDeviceToHost);
\end{lstlisting}

\paragraph{Оптимизации для CUDA}
\begin{itemize}
  \item Coalesced memory access: чтение подряд идущих адресов одним warp\;
  \item Shared memory для уменьшения обращений к глобальной памяти\;
  \item Асинхронное копирование и стримы (\texttt{cudaMemcpyAsync}, \texttt{cudaStream}) для перекрытия вычислений и передачи данных.
\end{itemize}

\paragraph{PyTorch и GPU}
PyTorch позволяет легко переносить тензоры на устройство и выполнять вычисления:

\begin{lstlisting}[language=Python,caption={PyTorch: базовые операции на GPU},label={lst:torch_gpu}]
import torch

t = torch.randn(1024, 1024, device='cuda')

y = torch.mm(t, t)

x = torch.tensor([1., 2., 3.], device='cuda', requires_grad=True)
y = x.pow(2).sum()
y.backward()
print(x.grad)  # tensor([2., 4., 6.], device='cuda:0')
\end{lstlisting}

\paragraph{Сравнение библиотек}
\textbf{TensorFlow} (статический граф):
\begin{lstlisting}[language=Python,caption={TensorFlow: определение и запуск функции},label={lst:tf}]
import tensorflow as tf

@tf.function
def fwd(x):
    return tf.matmul(x, x)

x = tf.random.normal((1024, 1024))
y = fwd(x)
\end{lstlisting}

\textbf{JAX} (JIT и векторизация):
\begin{lstlisting}[language=Python,caption={JAX: JIT-компиляция и градиент},label={lst:jax}]
import jax.numpy as jnp
from jax import jit, grad

@jit
def f(x):
    return jnp.dot(x, x)

res = f(jnp.ones((1024, 1024)))
g = grad(lambda m: jnp.sum(m**2))(jnp.ones((3,)))
\end{lstlisting}

\textbf{MXNet} (Gluon API):
\begin{lstlisting}[language=Python,caption={MXNet: dot-операция на GPU},label={lst:mxnet}]
from mxnet import nd, gpu

x = nd.random.normal(shape=(1024,1024), ctx=gpu())
w = nd.random.normal(shape=(1024,1024), ctx=gpu())
y = nd.dot(x, w)
\end{lstlisting}

PyTorch выделяется динамическим графом и удобством отладки, TensorFlow — оптимизацией статических моделей, JAX — простотой JIT и векторизации, MXNet — гибкостью Gluon API.

\subsection{Методы предварительной обработки и улучшения данных}

В современных задачах машинного обучения и анализа данных качество результатов во многом зависит от корректности и информативности исходных признаков. Процесс \emph{предварительной обработки} (preprocessing) включает в себя целый набор операций, направленных на приведение данных к удобному для моделей виду, снижение шумов и балансировку выборки. Ниже приведены три ключевых группы методов: масштабирование, синтетическое дополнение редких классов и работа с выбросами. Для каждой группы описаны цели, основные подходы, преимущества и ограничения, а также приведены примеры формул и кода на Python.

% ----------------------------------------
\subsubsection{Масштабирование}
\label{sec:scaling}

\paragraph{Зачем нужно масштабирование?}  
Многие алгоритмы (например, методы на основе евклидова расстояния или градиентного спуска) чувствительны к масштабу признаков. Если один признак принимает значения от 0 до 1, а другой — от \(10^3\) до \(10^6\), модель будет «смотреть» в первую очередь на крупномасштабный признак. Кроме того, плохая масштабировка может замедлять сходимость оптимизации.

\paragraph{Основные методы}
\begin{itemize}
  \item \textbf{Min–Max нормализация:}  
    \[
      x' = \frac{x - x_{\min}}{x_{\max} - x_{\min}},
      \quad x'\in[0,1].
    \]
    \emph{Плюсы:} сохраняет форму распределения, легко интерпретировать.  
    \emph{Минусы:} чувствительна к выбросам, требует знания глобальных минимумов и максимумов.
    
  \item \textbf{Z-преобразование (Standardization):}  
    \[
      x' = \frac{x - \mu}{\sigma},
      \quad \mu = \frac{1}{n}\sum_i x_i,\;
      \sigma = \sqrt{\frac{1}{n}\sum_i (x_i - \mu)^2}.
    \]
    \emph{Плюсы:} приводит данные к нулевому среднему и единичному стандартному отклонению, устойчиво при небольших выбросах.  
    \emph{Минусы:} всё ещё может страдать от сильных выбросов.
    
  \item \textbf{Десятичное масштабирование:}  
    \[
      x' = \frac{x}{10^j},\quad
      j = \left\lceil \log_{10}\bigl(\max_i |x_i|\bigr)\right\rceil.
    \]
    \emph{Плюсы:} просто реализуется, гарантирует \(|x'|<1\).  
    \emph{Минусы:} не выравнивает распределение, работает только с десятичной шкалой.
    
  \item \textbf{Робастное масштабирование:}  
    \[
      x' = \frac{x - \mathrm{median}(x)}{\mathrm{IQR}(x)},
      \quad \mathrm{IQR} = Q_3 - Q_1.
    \]
    \emph{Плюсы:} устойчиво к выбросам.  
    \emph{Минусы:} требует расчёта квартилей, менее распространено.
\end{itemize}

\paragraph{Снижение размерности как частный случай}  
Иногда под «масштабированием» понимают удаление избыточных признаков.  
\begin{itemize}
  \item \emph{PCA} (метод главных компонент): решается собственная задача для ковариационной матрицы и берутся \(k\) ведущих компонент:
    \[
      X' = XW,\quad W = [v_1,\dots,v_k],\quad \Sigma v_i = \lambda_i v_i.
    \]
  \item \emph{LDA} (линейный дискриминантный анализ): максимизация отношения межклассовой и внутриклассовой дисперсии.
  \item Нелинейные методы \emph{t-SNE}, \emph{UMAP} — для визуализации в 2–3D.
\end{itemize}

\paragraph{Пример на Python}
\begin{lstlisting}[language=Python]
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.decomposition import PCA

mm = MinMaxScaler(feature_range=(0,1))
X_mm = mm.fit_transform(X)

std = StandardScaler()
X_std = std.fit_transform(X)

pca = PCA(n_components=2, random_state=42)
X_pca = pca.fit_transform(X_std)
\end{lstlisting}

% ----------------------------------------
\subsubsection{Синтетическое дополнение редких классов}
\label{sec:oversampling}

\paragraph{Почему важно дополнять редкие классы?}  
В случае сильного дисбаланса модель может «игнорировать» малочисленные классы, отказываться им уделять внимание и выдавать предсказания лишь для большинства. Синтетическое дополнение (oversampling) помогает сгладить этот эффект, повысить чувствительность к редким событиям и улучшить общую сбалансированность.

\paragraph{Методы синтеза}
\begin{enumerate}
  \item \textbf{SMOTE} (Synthetic Minority Over-sampling Technique):  
    Для каждой точки \(x_i\) класса-минорити выбирается один из \(k\) ближайших соседей \(x_{\rm nn}\) и синтезируется:
    \[
      x_{\rm new} = x_i + \delta\,(x_{\rm nn} - x_i),\quad \delta\in[0,1].
    \]
  \item \textbf{Borderline-SMOTE}:  
    Генерация новых примеров только в «пограничных» зонах, где редкий класс пересекается с мажорити.
  \item \textbf{ADASYN}:  
    Учитывает плотность объектов, создаёт больше синтетических точек там, где редкий класс особенно редок.
  \item \textbf{GAN-базированные методы}:  
    Обучают генератор/дискриминатор для создания правдоподобных новых образцов.
\end{enumerate}

\paragraph{Плюсы и минусы}
\begin{itemize}
  \item \emph{Плюсы:}  
    \begin{itemize}
      \item Сглаживает дисбаланс без потери информации.  
      \item Увеличивает объём обучающей выборки, что может улучшить регуляризацию.
    \end{itemize}
  \item \emph{Минусы:}  
    \begin{itemize}
      \item Риск переобучения на синтетических данных.  
      \item При сложном многомерном распределении могут появиться «ненатуральные» точки.
    \end{itemize}
\end{itemize}

\paragraph{Пример на Python}
\begin{lstlisting}[language=Python]
from imblearn.over_sampling import SMOTE

smote = SMOTE(sampling_strategy='minority', k_neighbors=5, random_state=42)
X_res, y_res = smote.fit_resample(X, y)
\end{lstlisting}

% ----------------------------------------
\subsubsection{Обнаружение и устранение выбросов}
\label{sec:outliers}

\paragraph{Зачем искать выбросы?}  
Выбросы могут искажать оценки параметров (среднее, дисперсию), влиять на обучение моделей и ухудшать обобщающую способность. При этом не все «экстремальные» значения — ошибки: порой это важные редкие события.

\paragraph{Методы обнаружения}
\begin{itemize}
  \item \textbf{Статистические критерии:}
    \begin{itemize}
      \item Z-score: \(\displaystyle |z_i| > \tau\) (\(\tau\approx3\)).
      \item IQR-критерий: \(x < Q_1 - 1.5\,\mathrm{IQR}\) или \(x > Q_3 + 1.5\,\mathrm{IQR}\).
    \end{itemize}
  \item \textbf{Алгоритмические:}
    \begin{itemize}
      \item Isolation Forest: строит деревья, в которых «аномалии» изолируются быстрее.
      \item Local Outlier Factor: сравнивает плотность вокруг точки с плотностью у соседей.
      \item One-Class SVM: обучается только на «нормальных» данных и выявляет выбросы.
    \end{itemize}
\end{itemize}

\paragraph{Стратегии устранения}
\begin{itemize}
  \item \emph{Удаление:} простое отбрасывание аномальных строк.
  \item \emph{Замена:} подстановка медианы, моды или крайних значений.
  \item \emph{Капирование (capping):} обрезание выходящих за пределы значений до выбранных порогов.
\end{itemize}

\paragraph{Плюсы и минусы}
\begin{itemize}
  \item \emph{Плюсы:}  
    \begin{itemize}
      \item Повышает надёжность статистических оценок.  
      \item Облегчает обучение моделей, снижает риск переобучения на шуме.
    \end{itemize}
  \item \emph{Минусы:}  
    \begin{itemize}
      \item Можно случайно удалить важные редкие события.  
      \item Сложно задать универсальные пороги для разных признаков.
    \end{itemize}
\end{itemize}

\paragraph{Пример на Python}
\begin{lstlisting}[language=Python]
import numpy as np
from sklearn.ensemble import IsolationForest

z_scores = (X - X.mean(axis=0)) / X.std(axis=0)
mask = np.all(np.abs(z_scores) < 3, axis=1)
X_clean_z = X[mask]

iso = IsolationForest(contamination=0.05, random_state=0)
labels = iso.fit_predict(X)
X_clean_if = X[labels == 1]
\end{lstlisting}

% ----------------------------------------
% Конец раздела
% ----------------------------------------
% \subsection{Методы предварительной обработки и улучшения данных}
% \subsubsection{Масштабировани}
% \subsubsection{Синтетическое дополнение редких классов (SMOTE)}
% \subsubsection{Обнаружение и устранение выбросов}
%
\subsection{Инструменты и методы оптимизации гиперпараметров}
\label{sec:hyperparameter-optimization}

Под гиперпараметрами понимаются конфигурационные параметры модели или алгоритма, устанавливаемые до начала процесса обучения или инициализации и не обновляемые в процессе градиентного спуска или другого метода оптимизации. К классическим примерам относятся скорость обучения, глубина дерева решений, число скрытых слоёв нейросети и параметры регуляризации. Более того, в гибридных системах нейронно-нечеткой логики гиперпараметрами могут быть весовые коэффициенты правил нечеткого вывода, параметры функций принадлежности и пороги активации. Корректный подбор гиперпараметров напрямую влияет на качество работы модели, скорость её сходимости, устойчивость к переобучению и способность адаптироваться к сложным неопределённым условиям.

\subsubsection{Классические методы}

\paragraph{Перебор по сетке (Grid Search)}  
Пусть для каждого из \(k\) гиперпараметров задано дискретное множество \(P_i\), \(|P_i|=n_i\). Тогда полный перебор предполагает проверку всех комбинаций:
\begin{equation}
\mathcal{P} \;=\; P_1 \times P_2 \times \cdots \times P_k,
\qquad
|\mathcal{P}| \;=\; \prod_{i=1}^k n_i.
\label{eq:grid-search-size}
\end{equation}
\textbf{Плюсы:} простота реализации, гарантированное покрытие решётки.  
\textbf{Минусы:} экспоненциальный рост числа испытаний при увеличении \(k\) и \(n_i\).

\paragraph{Случайный поиск (Random Search)}  
При ограниченном бюджете \(N\) испытаний точки выбираются в гиперпространстве равномерно:
\begin{equation}
p_i^{(j)} \;\sim\; \mathcal{U}(a_i, b_i),
\quad j = 1,\dots,N.
\label{eq:random-search}
\end{equation}
\citet{bergstra2012random} показали, что при фиксированном \(N\) Random Search часто эффективнее Grid Search, так как распределяет усилия по более важным измерениям.

\subsubsection{Байесовская оптимизация}

Методы байесовской оптимизации строят суррогатную модель целевой функции
\(
f\colon \mathcal{X}\to\mathbb{R}
\)
на основе предыдущих наблюдений \(\mathcal{D}=\{(x_i, f(x_i))\}\) и используют функцию приобретения \(\alpha(x)\) для выбора следующей точки:
\begin{equation}
x_{n+1} = \arg\max_{x\in\mathcal{X}} \alpha\bigl(x;\,p(f\mid\mathcal{D})\bigr).
\label{eq:bayes-opt}
\end{equation}

\paragraph{Gaussian Process (GP)}  
Суррогатная модель задаётся процессом:
\begin{equation}
f(x)\sim\mathcal{GP}\bigl(m(x),\,k(x,x')\bigr),
\label{eq:gp-prior}
\end{equation}
где \(m(x)\) — функция среднего, \(k(x,x')\) — ковариационное ядро. По данным \(\mathcal{D}\) получают предсказание \(\mu_n(x)\) и неопределённость \(\sigma_n(x)\).

\paragraph{Функции приобретения}  
\begin{align}
\mathrm{EI}(x)
&= (f_{\min}-\mu_n(x))\,\Phi\bigl(Z\bigr)
+ \sigma_n(x)\,\phi\bigl(Z\bigr),
\quad Z = \frac{f_{\min}-\mu_n(x)}{\sigma_n(x)},
\label{eq:expected-improvement}\\
\mathrm{PI}(x)
&= \Phi\!\Bigl(\frac{f_{\min}-\mu_n(x)}{\sigma_n(x)}\Bigr),
\label{eq:probability-improvement}\\
\mathrm{UCB}(x)
&= \mu_n(x) - \kappa\,\sigma_n(x).
\label{eq:upper-conf-bound}
\end{align}

\subsubsection{Адаптивное распределение ресурсов}

\paragraph{Successive Halving}  
Все \(n\) конфигураций обучаются на малом бюджете \(r\), затем отбираются лучшие \(n/\eta\) и их обучают с бюджетом \(\eta r\), и так далее.

\paragraph{Hyperband / ASHA}  
Комбинирует Successive Halving с несколькими начальными бюджетами \(r_1, r_2, \dots, r_s\), что позволяет балансировать между шириной и глубиной поиска.

\subsubsection{Современные фреймворки}

\begin{itemize}
  \item \textbf{Optuna} \cite{akiba2019optuna}. Основан на TPE (Tree-structured Parzen Estimator).
  \item \textbf{Hyperopt}. Реализует TPE и Random Search, конфигурируется через Python-словари.
  \item \textbf{Scikit-Optimize (skopt)}. Предоставляет \texttt{gp\_minimize}, \texttt{forest\_minimize}, \texttt{gbrt\_minimize}.
  \item \textbf{Ray Tune}. Распределённая оптимизация с поддержкой HyperOpt, BayesOpt, HyperBand.
  \item \textbf{SMAC}. Bayesian Optimization на основе Random Forest.
\end{itemize}

\subsubsection{Пример использования Optuna}

\begin{lstlisting}[language=Python]
import optuna

def objective(trial):
    lr = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)
    n_estimators = trial.suggest_int('n_estimators', 50, 500)
    score = train_and_evaluate(lr, n_estimators)
    return score

study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=50)
\end{lstlisting}

\subsubsection{Рекомендации по выбору метода}

\begin{itemize}
  \item Для небольшого числа параметров (\(<5\)) и невысокой нагрузки — Grid Search (см. формулу \eqref{eq:grid-search-size}).
  \item При ограниченном бюджете и большом числе параметров — Random Search (\eqref{eq:random-search}) или Optuna/TPE.
  \item Для вычислительно тяжёлых моделей — Hyperband/ASHA в сочетании с Bayesian Optimization (\eqref{eq:bayes-opt}).
\end{itemize}
