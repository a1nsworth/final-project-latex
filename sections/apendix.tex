\ESKDthisStyle{formII}
\addcontentsline{toc}{section}{СПИСОК ЛИТЕРАТУРЫ} % и добавляем её в оглавление, если требуется
\ESKDsignature{\normalsize Список литературы}
\eskdrerun{}

\begin{thebibliography}{200}
\bibitem{Kulabukhov2023}
С.\,В.~Кулабухов.
\newblock Разработка методов анализа данных на основе мягких вычислений
  с использованием нечеткого значения истинности.
\newblock Кандидатская диссертация, БГТУ им.~В.\,Г.~Шухова, Белгород, 2023.

\bibitem{SinukPivnenko2006}
В.\,Г.~Синюк, Е.\,В.~Пивненко.
\newblock Об аналитическом вычислении нечеткого значения истинности.
\newblock В сб.: Нечеткие системы, мягкие вычисления и интеллектуальные технологии (НСМВ-2006), Физматлит, М., 2006, с.~129–133.
\bibitem{Zadeh1965}
  Заде Л.~А. \emph{Fuzzy sets} // Information and Control. 1965. Т.~8, №~3. С.~338--353.
\bibitem{Mamdani1975}
  Мамдани Э.~Х., Ассилиан С. \emph{An experiment in linguistic synthesis with a fuzzy logic controller} // International Journal of Man-Machine Studies. 1975. Т.~7, №~1. С.~1--13.
\bibitem{Ross2004}
  Росс Т.~Дж. \emph{Fuzzy Logic with Engineering Applications}. Chichester: John Wiley \& Sons, 2004. 576 с.
\bibitem{Zadeh1975}
  Заде Л.~А. \emph{The concept of a linguistic variable and its application to approximate reasoning—I} // Information Sciences. 1975. Т.~8, №~4. С.~199--249.
\bibitem{Sugeno1985}
  Сугено М. \emph{Industrial applications of fuzzy control} // Fuzzy Sets and Systems. 1985. Т.~29, №~1. С.~47--52.
\bibitem{Jang1993}
  Джанг Дж.-С.~Р. \emph{ANFIS: Adaptive-Network-Based Fuzzy Inference System} // IEEE Trans. Syst., Man, and Cybernetics. 1993. Т.~23, №~3. С.~665--685.
\bibitem{Wang1992}
  Ван Л.-Х., Мендель Дж.-М. \emph{Generating fuzzy rules by learning from examples} // IEEE Trans. Syst., Man, and Cybernetics. 1992. Т.~22, №~6. С.~1414--1427.
\bibitem{Bezdek1981}
  Бездек Дж.~К. \emph{Pattern Recognition with Fuzzy Objective Function Algorithms}. New York: Plenum Press, 1981. 271 с.

\bibitem{Zadeh1965}
L.\,A.~Zadeh.
\newblock Fuzzy Sets.
\newblock {\em Information and Control}, 8(3):338–353, 1965.

\bibitem{bergstra2012random}
  Бергстра Дж., Бенжио Й. \emph{Random Search for Hyper-Parameter Optimization} // Journal of Machine Learning Research. 2012. Т.~13, №~10. С.~281--305.
  
\bibitem{Bezdek1981}
J.\,C.~Bezdek.
\newblock Pattern Recognition with Fuzzy Objective Function Algorithms.
\newblock Springer, 1981.

\bibitem{Abe2001}
S.~Abe.
\newblock Fuzzy Logic for Engineering Applications.
\newblock Springer, 2001.

\bibitem{Mamdani1974}
E.\,H.~Mamdani.
\newblock Applications of Fuzzy Algorithms for Simple Dynamic Plants.
\newblock {\em Proceedings of the IEEE}, 121(1):1585–1588, 1974.

\bibitem{TakagiSugeno1985}
T.~Takagi, M.~Sugeno.
\newblock Fuzzy Identification of Systems and Its Applications to Modeling and Control.
\newblock {\em IEEE Trans. Systems, Man, and Cybernetics}, SMC-15(1):116–132, 1985.
\end{thebibliography}

\newpage
\section*{ПРИЛОЖЕНИЕ A}
\addcontentsline{toc}{section}{ПРИЛОЖЕНИЕ A}
\ESKDsignature{\normalsize Приложение A}
\eskdrerun{}
\begin{minted}{python}
import argparse
import io
import urllib.request

import numpy as np
import pandas as pd
import torch
import optuna

from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import accuracy_score, classification_report
from sklearn.datasets import (
    load_iris,
    load_wine,
    load_breast_cancer,
    load_digits
)

# Параметры
DATASET      = "wine"
N_TERMS      = 3
CV_FOLDS     = 3
DATA_AUG_STD = 0.05
N_TRIALS     = 700
SEED         = 42

device = "cuda" if torch.cuda.is_available() else "cpu"
TAU    = torch.linspace(0.001, 1.0, 100, device=device)


def load_dataset(name):
    """Загрузка стандартных и UCI датасетов."""
    if name == "iris":
        ds = load_iris()
        return ds.data, ds.target, ds.feature_names, ds.target_names

    if name == "wine":
        ds = load_wine()
        return ds.data, ds.target, ds.feature_names, ds.target_names

    if name == "breast_cancer":
        ds = load_breast_cancer()
        return ds.data, ds.target, ds.feature_names, ds.target_names

    if name == "digits":
        ds = load_digits()
        X = ds.data.astype(np.float32)
        names = [f"px{i}" for i in range(64)]
        return X, ds.target, names, np.array([str(i) for i in range(10)])

    if name == "wine_red":
        url = (
            "https://archive.ics.uci.edu/ml/machine-learning-databases/"
            "wine-quality/winequality-red.csv"
        )
        raw = urllib.request.urlopen(url).read()
        df = pd.read_csv(io.BytesIO(raw), sep=";")
        X = df.iloc[:, :-1].values.astype(np.float32)
        y = df["quality"].values
        return X, y, list(df.columns[:-1]), np.array(sorted(np.unique(y)))

    raise ValueError(f"Unknown dataset: {name}")


X_all, y_all, feature_names, target_names = load_dataset(DATASET)
X_all = MinMaxScaler().fit_transform(X_all).astype(np.float32)

X_tr_raw, X_te, y_tr_raw, y_te = train_test_split(
    X_all, y_all,
    stratify=y_all,
    test_size=0.2,
    random_state=SEED
)

X_te_tensor = torch.tensor(X_te, device=device)
n_feat = X_tr_raw.shape[1]
n_cls  = len(np.unique(y_all))


def mu_cp(a1, a2, sigma_in, sigma_noise):
    """Совместимость двух гауссиан (без нормировки)."""
    a1 = a1.float()
    a2 = a2.float()
    sigma = (
        sigma_in.float()
        if torch.is_tensor(sigma_in)
        else torch.tensor(sigma_in, device=device)
    )
    diff = (a1 - a2).unsqueeze(1)
    st   = torch.sqrt(-2 * torch.log(TAU)).unsqueeze(0)
    num  = torch.where(
        (a1 <= a2).unsqueeze(1),
        diff - sigma * st,
        diff + sigma * st
    )
    return torch.exp(-(num**2) / (2 * sigma_noise**2)).float()


def opportunity(a1, a2, sigma_in, sigma_noise):
    """Нечёткая степень истинности τ = max_τ min(μ_cp, τ)."""
    mu = mu_cp(a1, a2, sigma_in, sigma_noise)
    return torch.max(torch.minimum(mu, TAU), dim=1).values


def build_vec(trial):
    """Строим вектор параметров для Optuna."""
    vec = []
    for f in range(n_feat):
        for t in range(N_TERMS):
            vec.append(trial.suggest_float(f"mu_{f}_{t}", 0.01, 10, log=True))
            vec.append(trial.suggest_float(f"sig_{f}_{t}", 0.01, 10, log=True))

    for c in range(n_cls):
        for f in range(n_feat):
            vec.append(trial.suggest_int(f"term_{c}_{f}", 0, N_TERMS - 1))

    for c in range(n_cls):
        vec.append(trial.suggest_float(f"c_out_{c}", 0.05, 10, log=True))
        vec.append(trial.suggest_float(f"s_out_{c}", 0.05, 10, log=True))

    return vec


def predict(vec, xb, noise):
    """Нечёткий вывод и предсказание класса."""
    v = torch.tensor(vec, dtype=torch.float32, device=device)
    ptr = 0

    mu   = v[ptr : ptr + n_feat * N_TERMS].view(n_feat, N_TERMS)
    ptr += n_feat * N_TERMS

    sig  = v[ptr : ptr + n_feat * N_TERMS].view(n_feat, N_TERMS)
    ptr += n_feat * N_TERMS

    terms = v[ptr : ptr + n_cls * n_feat].long().view(n_cls, n_feat)
    ptr += n_cls * n_feat

    c_out = v[ptr : ptr + n_cls]
    s_out = v[ptr + n_cls : ptr + 2 * n_cls]

    B   = xb.size(0)
    Pis = torch.zeros(B, n_cls, n_feat, device=device)

    for c in range(n_cls):
        for i in range(n_feat):
            idx = terms[c, i]
            Pis[:, c, i] = opportunity(
                mu[i, idx].expand(B),
                xb[:, i],
                sig[i, idx],
                noise
            )

    fire = Pis.min(dim=2).values
    num  = (fire * s_out).sum(1)
    den  = fire.sum(1) + 1e-12
    y_hat = (num / den).round().clamp(0, n_cls - 1)

    return y_hat.cpu().numpy().astype(int)


def objective(trial):
    """Функция для оптимизации Optuna (минимизировать 1 − accuracy)."""
    vec = build_vec(trial)
    skf = StratifiedKFold(
        n_splits=CV_FOLDS,
        shuffle=True,
        random_state=SEED
    )

    accs = []
    for _, val_idx in skf.split(X_tr_raw, y_tr_raw):
        X_val = X_tr_raw[val_idx]
        y_val = y_tr_raw[val_idx]
        noisy = X_val + np.random.randn(*X_val.shape) * DATA_AUG_STD
        preds = predict(vec, torch.tensor(noisy, device=device), DATA_AUG_STD)
        accs.append(accuracy_score(y_val, preds))

    return 1 - np.mean(accs)


if __name__ == "__main__":
    study = optuna.create_study(direction="minimize")
    study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=True)

    best = build_vec(study.best_trial)
    y_pred = predict(best, X_te_tensor, DATA_AUG_STD)

    print(classification_report(y_te, y_pred, target_names=target_names))
    print("Accuracy:", accuracy_score(y_te, y_pred))
\end{minted}

